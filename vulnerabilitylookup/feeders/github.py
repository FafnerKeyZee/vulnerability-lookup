import json
import logging
import re
import sys

from datetime import datetime, timezone
from pathlib import Path
from typing import Set, List, Dict

from git import Repo
from redis import Redis

from ..default import get_config, get_homedir
from ..helpers import get_config_feeder


class GithubAdvisoryDatabase():
    def __init__(self):
        self.logger = logging.getLogger(f'{self.__class__.__name__}')
        self.config = get_config_feeder('gad')

        if 'level' in self.config:
            self.logger.setLevel(self.config['level'])
        else:
            self.logger.setLevel(get_config('generic', 'loglevel'))

        self.storage = Redis(host=get_config('generic', 'storage_db_hostname'),
                             port=get_config('generic', 'storage_db_port'))

        root_repo = Repo(get_homedir())
        root_repo.submodule('advisory-database').update(init=True)

        self.path_to_repo = get_homedir() / 'vulnerabilitylookup' / 'feeders' / 'advisory-database'
        self.gad_git = Repo(self.path_to_repo)

    def gad_update(self) -> bool:

        self.gad_git.remotes.origin.pull('main')

        paths_to_import: Set[Path] = set()
        if _last_update := self.storage.hget('last_updates', 'gad'):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.gad_git.head.commit.hexsha:
                # No changes
                return False
            changed_paths: List[str] = []
            for commit in self.gad_git.iter_commits(f'{_last_update_str}...HEAD'):
                changed_paths += self.gad_git.git.show(commit.hexsha, name_only=True).split('\n')
            # make sure we only get the paths we care about
            for _p in set(changed_paths):
                p_path = self.path_to_repo / Path(_p)
                if p_path.suffix == '.json' and re.match('GHSA(-[23456789cfghjmpqrvwx]{4}){3}', p_path.stem):
                    paths_to_import.add(p_path)
        else:
            # First run, get all files
            for year_dir in (sorted((self.path_to_repo / 'advisories' / 'github-reviewed').iterdir())
                             + sorted((self.path_to_repo / 'advisories' / 'unreviewed').iterdir())):
                if not year_dir.name.isdigit():
                    continue
                for month_dir in sorted(year_dir.iterdir()):
                    if not month_dir.name.isdigit():
                        continue
                    for entry in sorted(month_dir.iterdir()):
                        if not re.match('GHSA(-[23456789cfghjmpqrvwx]{4}){3}', entry.stem):
                            continue
                        if (entry / f'{entry.stem}.json').exists():
                            paths_to_import.add(entry / f'{entry.stem}.json')

        if not paths_to_import:
            return False

        p = self.storage.pipeline()
        gsids: Dict[str, float] = {}
        for path in paths_to_import:
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = json.load(vuln_entry)
                if sys.version_info >= (3, 11):
                    modified = datetime.fromisoformat(vuln['modified'])
                else:
                    modified = datetime.strptime(vuln['modified'], '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)
                gsids[path.stem] = modified.timestamp()
                if 'aliases' in vuln and vuln.get('aliases'):
                    for alias in vuln.get('aliases'):
                        p.sadd(f'{path.stem}:link', alias)
                        p.sadd(f'{alias}:link', path.stem)
                p.set(path.stem, json.dumps(vuln))
            if len(gsids) > 1000:
                # Avoid a massive execute on fitrst import
                p.zadd('index:gad', gsids)  # type: ignore
                p.zadd('index', gsids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                gsids = {}

        if gsids:
            # remaining entries
            p.zadd('index:gad', gsids)  # type: ignore
            p.zadd('index', gsids)  # type: ignore
            p.execute()
        self.storage.hset('last_updates', mapping={'gad': self.gad_git.head.commit.hexsha})
        return True
