import json

from pathlib import Path
from typing import Set, Dict

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class GSD(AbstractFeeder):
    def __init__(self):
        super().__init__(Path(__file__).stem)

        self.init_git_repo()

    def update(self) -> bool:

        self.git.remotes.origin.pull('main')

        paths_to_import: Set[Path] = set()
        if _last_update := self.storage.hget('last_updates', self.name):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.git.head.commit.hexsha:
                # No changes
                self.logger.info('No new commit.')
                return False
            for commit in self.git.iter_commits(f'{_last_update_str}...HEAD'):
                for line in self.git.git.show(commit.hexsha, name_only=True).split('\n'):
                    if not line.endswith('.json'):
                        continue
                    p_path = self.path_to_repo / Path(line)
                    if p_path.exists() and p_path.parent.name.endswith('xxx') and p_path.parent.parent.name.isdigit():
                        paths_to_import.add(p_path)
        else:
            # First run, get all files
            for year_dir in sorted(self.path_to_repo.iterdir()):
                if not year_dir.name.isdigit():
                    continue
                for vuln_dir in sorted(year_dir.iterdir()):
                    if not vuln_dir.name.endswith('xxx'):
                        continue
                    for entry in sorted(vuln_dir.iterdir()):
                        if entry.suffix != '.json':
                            continue
                        paths_to_import.add(entry)

        if not paths_to_import:
            self.logger.info('Nothing new to import.')
            return False

        p = self.storage.pipeline()
        gsids: Dict[str, float] = {}
        for path in paths_to_import:
            needs_lastmodified_from_git = False
            last_modified = None
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = json.load(vuln_entry)
                # NOTE 2023-09-06: the GSD file format doesn't contains a modified timestamp
                # but the sources seem to kinda have one, most of the time, maybe.
                if 'gsd' in vuln:
                    if 'modified' in vuln['gsd']:
                        last_modified = fromisoformat_wrapper(vuln['gsd']['modified'])
                    elif 'osvSchema' in vuln['gsd']:
                        last_modified = fromisoformat_wrapper(vuln['gsd']['osvSchema']['modified'])
                if not last_modified and 'namespaces' in vuln:
                    if 'nvd.nist.gov' in vuln['namespaces']:
                        try:
                            last_modified = fromisoformat_wrapper(vuln['namespaces']['nvd.nist.gov']['lastModifiedDate'])
                        except ValueError as e:
                            # nothing important, just a broken date
                            self.logger.warning(f'Unable to get modified date: {e}')
                            needs_lastmodified_from_git = True
                    elif 'gitlab.com' in vuln['namespaces']:
                        try:
                            last_modified = fromisoformat_wrapper(vuln['namespaces']['gitlab.com']['advisories'][0]['date'])
                        except ValueError as e:
                            # nothing important, just a broken date
                            self.logger.warning(f'Unable to get modified date: {e}')
                            needs_lastmodified_from_git = True
                    else:
                        if 'cve.org' in vuln['namespaces'] and vuln['namespaces']['cve.org']['CVE_data_meta']['STATE'] == 'RESERVED':
                            # reserved ID
                            needs_lastmodified_from_git = True

                if not last_modified and 'OSV' in vuln:
                    if 'modified' in vuln['OSV']:
                        # Python < 3.11 cannot load times with a Z instead of +00:00

                        last_modified = fromisoformat_wrapper(vuln['OSV']['modified'])
                    elif 'withdrawn' in vuln['OSV']:
                        last_modified = fromisoformat_wrapper(vuln['OSV']['withdrawn'])

                if not last_modified and 'GSD' in vuln:
                    if 'modified' in vuln['GSD']:
                        last_modified = fromisoformat_wrapper(vuln['GSD']['modified'])
                    elif 'withdrawn' in vuln['GSD']:
                        last_modified = fromisoformat_wrapper(vuln['GSD']['withdrawn'])
                    else:
                        needs_lastmodified_from_git = True

                if not last_modified:
                    if not needs_lastmodified_from_git:
                        self.logger.warning(f'Unable to process {path}, please have a look yourself, good luck!')
                        continue

                    # NOTE old approach: there is no indication when the entry was last updated in the json,
                    # using the last time that file was commited
                    # It is slow as hell, but that's the best we can do.

                    commit = next(self.git.iter_commits(max_count=1, paths=path))
                    last_modified = commit.committed_datetime

                gsids[path.stem] = last_modified.timestamp()
                if 'GSD' in vuln and 'alias' in vuln['GSD']:
                    p.sadd(f'{path.stem}:link', vuln['GSD']['alias'])
                    p.sadd(f'{vuln["GSD"]["alias"]}:link', path.stem)
                p.set(path.stem, json.dumps(vuln))

            if len(gsids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f'index:{self.name}', gsids)  # type: ignore
                p.zadd('index', gsids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                gsids = {}

        if gsids:
            # remaining entries
            p.zadd(f'index:{self.name}', gsids)  # type: ignore
            p.zadd('index', gsids)  # type: ignore
            p.execute()
        self.storage.hset('last_updates', mapping={self.name: self.git.head.commit.hexsha})
        self.logger.info('Import done.')
        return True
