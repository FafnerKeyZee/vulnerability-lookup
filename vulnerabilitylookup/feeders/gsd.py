import json
import logging

from pathlib import Path
from typing import Set, List

from git import Repo
from redis import Redis

from ..default import get_config, get_homedir
from ..helpers import get_config_feeder


class GSD():
    def __init__(self):
        self.logger = logging.getLogger(f'{self.__class__.__name__}')
        self.config = get_config_feeder('gsd')

        if 'level' in self.config:
            self.logger.setLevel(self.config['level'])
        else:
            self.logger.setLevel(get_config('generic', 'loglevel'))

        self.storage = Redis(host=get_config('generic', 'storage_db_hostname'),
                             port=get_config('generic', 'storage_db_port'))

        self.path_to_repo = get_homedir() / 'vulnerabilitylookup' / 'feeders' / 'gsd-database'
        self.gsd_git = Repo(self.path_to_repo)
        for submodule in self.gsd_git.submodules:
            submodule.update(init=True)

    def gsd_update(self) -> bool:

        self.gsd_git.remotes.origin.pull()

        paths_to_import: Set[Path] = set()
        if _last_update := self.storage.hget('last_updates', 'gsd'):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.gsd_git.head.commit.hexsha:
                # No changes
                return False
            changed_paths: List[str] = []
            for commit in self.gsd_git.iter_commits(f'{_last_update_str}...HEAD'):
                changed_paths += self.gsd_git.git.show(commit.hexsha, name_only=True).split('\n')
            # make sure we only get the paths we care about
            for _p in set(changed_paths):
                p_path = self.path_to_repo / Path(_p)
                if p_path.suffix == '.json' and p_path.parent.name.endswith('xxx') and p_path.parent.parent.name.isdigit():
                    paths_to_import.add(p_path)
        else:
            # First run, get all files
            for year_dir in sorted(self.path_to_repo.iterdir()):
                if not year_dir.name.isdigit():
                    continue
                for vuln_dir in sorted(year_dir.iterdir()):
                    if not vuln_dir.name.endswith('xxx'):
                        continue
                    for entry in sorted(vuln_dir.iterdir()):
                        if entry.suffix != '.json':
                            continue
                        paths_to_import.add(entry)

        if not paths_to_import:
            return False

        p = self.storage.pipeline()
        gsids = set()
        for path in paths_to_import:
            # Store all cves individually
            gsids.add(path.stem)
            with path.open() as vuln_entry:
                vuln = json.load(vuln_entry)
                if 'GSD' in vuln and 'alias' in vuln['GSD']:
                    p.sadd(f'{path.stem}:link', vuln['GSD']['alias'])
                    p.sadd(f'{vuln["GSD"]["alias"]}:link', path.stem)
                p.set(path.stem, json.dumps(vuln))
            if len(gsids) > 1000:
                # Avoid a massive execute on fitrst import
                p.sadd('index:gsd', *gsids)
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                gsids = set()

        if gsids:
            # remaining entries
            p.sadd('index:gsd', *gsids)
            p.execute()
        self.storage.hset('last_updates', mapping={'gsd': self.gsd_git.head.commit.hexsha})
        return True
