import json
import logging
import logging.config

from datetime import datetime
from pathlib import Path
from typing import Set, Dict

from git import Repo
from redis import Redis

from ..default import get_config, get_homedir
from ..helpers import get_config_feeder


class GSD():
    def __init__(self):
        self._load_logging_config()
        self.logger = logging.getLogger(f'{self.__class__.__name__}')
        self.config = get_config_feeder('gsd')

        if 'level' in self.config:
            self.logger.setLevel(self.config['level'])
        else:
            self.logger.setLevel(get_config('generic', 'loglevel'))

        self.storage = Redis(host=get_config('generic', 'storage_db_hostname'),
                             port=get_config('generic', 'storage_db_port'))

        root_repo = Repo(get_homedir())
        root_repo.submodule('gsd-database').update(init=True)

        self.path_to_repo = get_homedir() / 'vulnerabilitylookup' / 'feeders' / 'gsd-database'
        self.gsd_git = Repo(self.path_to_repo)

    def _load_logging_config(self):
        cur_path = Path(__file__)
        if not (cur_path.parent / f'{cur_path.stem}_logging.json').exists():
            return
        with (cur_path.parent / f'{cur_path.stem}_logging.json').open() as f:
            log_config = json.load(f)
        logging.config.dictConfig(log_config)

    def gsd_update(self) -> bool:

        self.gsd_git.remotes.origin.pull('main')

        paths_to_import: Set[Path] = set()
        if _last_update := self.storage.hget('last_updates', 'gsd'):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.gsd_git.head.commit.hexsha:
                # No changes
                self.logger.info('No new commit.')
                return False
            for commit in self.gsd_git.iter_commits(f'{_last_update_str}...HEAD'):
                for line in self.gsd_git.git.show(commit.hexsha, name_only=True).split('\n'):
                    if not line.endswith('.json'):
                        continue
                    p_path = self.path_to_repo / Path(line)
                    if p_path.exists() and p_path.parent.name.endswith('xxx') and p_path.parent.parent.name.isdigit():
                        paths_to_import.add(p_path)
        else:
            # First run, get all files
            for year_dir in sorted(self.path_to_repo.iterdir()):
                if not year_dir.name.isdigit():
                    continue
                for vuln_dir in sorted(year_dir.iterdir()):
                    if not vuln_dir.name.endswith('xxx'):
                        continue
                    for entry in sorted(vuln_dir.iterdir()):
                        if entry.suffix != '.json':
                            continue
                        paths_to_import.add(entry)

        if not paths_to_import:
            self.logger.info('Nothing new to import.')
            return False

        p = self.storage.pipeline()
        gsids: Dict[str, float] = {}
        for path in paths_to_import:
            needs_lastmodified_from_git = False
            last_modified = None
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = json.load(vuln_entry)
                # NOTE 2023-09-06: the GSD file format doesn't contains a modified timestamp
                # but the sources seem to kinda have one, most of the time, maybe.
                if 'gsd' in vuln:
                    if 'modified' in vuln['gsd']:
                        last_modified = datetime.fromisoformat(vuln['gsd']['modified'])
                    elif 'osvSchema' in vuln['gsd']:
                        last_modified = datetime.fromisoformat(vuln['gsd']['osvSchema']['modified'])
                if not last_modified and 'namespaces' in vuln:
                    if 'nvd.nist.gov' in vuln['namespaces']:
                        last_modified = datetime.fromisoformat(vuln['namespaces']['nvd.nist.gov']['lastModifiedDate'])
                    elif 'gitlab.com' in vuln['namespaces']:
                        last_modified = datetime.fromisoformat(vuln['namespaces']['gitlab.com']['advisories'][0]['date'])
                    else:
                        if 'cve.org' in vuln['namespaces'] and vuln['namespaces']['cve.org']['CVE_data_meta']['STATE'] == 'RESERVED':
                            # reserved ID
                            needs_lastmodified_from_git = True

                if not last_modified and 'OSV' in vuln:
                    if 'modified' in vuln['OSV']:
                        last_modified = datetime.fromisoformat(vuln['OSV']['modified'])
                    elif 'withdrawn' in vuln['OSV']:
                        last_modified = datetime.fromisoformat(vuln['OSV']['withdrawn'])

                if not last_modified and 'GSD' in vuln:
                    if 'modified' in vuln['GSD']:
                        last_modified = datetime.fromisoformat(vuln['GSD']['modified'])
                    elif 'withdrawn' in vuln['GSD']:
                        last_modified = datetime.fromisoformat(vuln['GSD']['withdrawn'])
                    else:
                        needs_lastmodified_from_git = True

                if not last_modified:
                    if not needs_lastmodified_from_git:
                        self.logger.warning(f'Unable to process {path}, please have a look yourself, good luck!')
                        continue

                    # NOTE old approach: there is no indication when the entry was last updated in the json,
                    # using the last time that file was commited
                    # It is slow as hell, but that's the best we can do.

                    commit = next(self.gsd_git.iter_commits(max_count=1, paths=path))
                    last_modified = commit.committed_datetime

                gsids[path.stem] = last_modified.timestamp()
                if 'GSD' in vuln and 'alias' in vuln['GSD']:
                    p.sadd(f'{path.stem}:link', vuln['GSD']['alias'])
                    p.sadd(f'{vuln["GSD"]["alias"]}:link', path.stem)
                p.set(path.stem, json.dumps(vuln))

            if len(gsids) > 1000:
                # Avoid a massive execute on first import
                p.zadd('index:gsd', gsids)  # type: ignore
                p.zadd('index', gsids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                gsids = {}

        if gsids:
            # remaining entries
            p.zadd('index:gsd', gsids)  # type: ignore
            p.zadd('index', gsids)  # type: ignore
            p.execute()
        self.storage.hset('last_updates', mapping={'gsd': self.gsd_git.head.commit.hexsha})
        self.logger.info('Import done.')
        return True
