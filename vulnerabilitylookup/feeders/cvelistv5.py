#!/usr/bin/env python3

import json
import re

from pathlib import Path
from typing import Dict, Set

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class CVEListV5(AbstractFeeder):
    def __init__(self):
        super().__init__(Path(__file__).stem)

        self.init_git_repo()

    def update(self) -> bool:
        self.git.remotes.origin.pull('main')

        paths_to_import: Set[Path] = set()
        if _last_update := self.storage.hget('last_updates', self.name):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.git.head.commit.hexsha:
                # No changes
                self.logger.info('No new commit.')
                return False
            for commit in self.git.iter_commits(f'{_last_update_str}...HEAD'):
                for line in self.git.git.show(commit.hexsha, name_only=True).split('\n'):
                    if not line.endswith('.json'):
                        continue
                    p_path = self.path_to_repo / Path(line)
                    if p_path.exists() and re.match(r'CVE-\d{4}-\d+.json', p_path.name):
                        paths_to_import.add(p_path)
        else:
            # First run, get all files
            for p_path in self.path_to_repo.rglob('*.json'):
                if p_path.exists() and re.match(r'CVE-\d{4}-\d+.json', p_path.name):
                    paths_to_import.add(p_path)

        if not paths_to_import:
            self.logger.info('Nothing new to import.')
            return False

        p = self.storage.pipeline()
        cvelistv5ids: Dict[str, float] = {}
        for path in paths_to_import:
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = json.load(vuln_entry)
                if 'dateUpdated' in vuln['cveMetadata']:
                    updated = fromisoformat_wrapper(vuln['cveMetadata']['dateUpdated'])
                elif 'datePublished' in vuln['cveMetadata']:
                    updated = fromisoformat_wrapper(vuln['cveMetadata']['datePublished'])
                else:  # 'dateReserved' in vuln['cveMetadata']:
                    updated = fromisoformat_wrapper(vuln['cveMetadata']['dateReserved'])

                cvelistv5ids[path.stem] = updated.timestamp()
                p.set(path.stem, json.dumps(vuln))
                # Check if we have a link with another known source we can link to
                if ('containers' in vuln
                        and 'cna' in vuln['containers']
                        and 'source' in vuln['containers']['cna']
                        and 'advisory' in vuln['containers']['cna']['source']):
                    if vuln['containers']['cna']['source']['advisory'].startswith('GHSA'):
                        # got a github security advisory.
                        p.sadd(f'{path.stem}:link', vuln['containers']['cna']['source']['advisory'])
                        p.sadd(f"{vuln['containers']['cna']['source']['advisory']}:link", path.stem)
                    else:
                        self.logger.debug(f"[{path.stem}] Unknown advisory ID: {vuln['containers']['cna']['source']['advisory']}")

                # Load affected products
                if ('containers' in vuln
                        and 'cna' in vuln['containers']
                        and 'affected' in vuln['containers']['cna']):
                    if ('vendor' in vuln['containers']['cna']['affected']
                            and 'product' in vuln['containers']['cna']['affected']):
                        vendor = vuln['containers']['cna']['affected']['vendor']
                        product = vuln['containers']['cna']['affected']['product']
                        p.sadd('vendors', vendor)
                        p.sadd(f'{vendor}:products', product)
                        p.sadd(f'{vendor}:vulnerabilities', path.stem)
                        p.sadd(f'{vendor}:{product}:vulnerabilities', path.stem)

            if len(cvelistv5ids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f'index:{self.name}', cvelistv5ids)  # type: ignore
                p.zadd('index', cvelistv5ids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                cvelistv5ids = {}
        if cvelistv5ids:
            # remaining entries
            p.zadd(f'index:{self.name}', cvelistv5ids)  # type: ignore
            p.zadd('index', cvelistv5ids)  # type: ignore
            p.execute()

        self.storage.hset('last_updates', mapping={self.name: self.git.head.commit.hexsha})
        self.logger.info('Import done.')
        return True
